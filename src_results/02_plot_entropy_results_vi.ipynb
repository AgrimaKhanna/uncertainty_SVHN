{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pathlib\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "# Setting the seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Evaluate the model with VI and calculate entropy\n",
    "def VI_dropout_predict_with_entropy(model, inputs, n_samples=10):\n",
    "    # model.train()  # Enable dropout during prediction\n",
    "    outputs = torch.stack([model(inputs) for _ in range(n_samples)])\n",
    "    mean_output = outputs.mean(dim=0)\n",
    "    uncertainty = outputs.var(dim=0)\n",
    "    # Calculate entropy\n",
    "    probs = F.softmax(mean_output, dim=1)  # Convert logits to probabilities\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=1)  # Add epsilon to avoid log(0)\n",
    "    # Getting the max probability\n",
    "    max_probs, _ = torch.max(probs, dim=1)\n",
    "    return mean_output, uncertainty, entropy, max_probs\n",
    "\n",
    "class FlipoutLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.prior_sigma = prior_sigma\n",
    "        \n",
    "        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features).normal_(0, 0.01))\n",
    "        self.weight_rho = nn.Parameter(torch.zeros(out_features, in_features).normal_(-5, 0.01))\n",
    "        self.bias_mu = nn.Parameter(torch.zeros(out_features).normal_(0, 0.01))\n",
    "        self.bias_rho = nn.Parameter(torch.zeros(out_features).normal_(-5, 0.01))\n",
    "        \n",
    "        # Random signs for flipout\n",
    "        self.register_buffer('input_sign', None)\n",
    "        self.register_buffer('output_sign', None)\n",
    "        \n",
    "    def get_random_signs(self, batch_size, shape, device):\n",
    "        return (2 * torch.bernoulli(torch.ones(batch_size, *shape, device=device) * 0.5) - 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if (self.input_sign is None or \n",
    "            self.input_sign.size(0) != batch_size or \n",
    "            self.output_sign is None or \n",
    "            self.output_sign.size(0) != batch_size):\n",
    "            \n",
    "            self.input_sign = self.get_random_signs(batch_size, (self.in_features,), x.device)\n",
    "            self.output_sign = self.get_random_signs(batch_size, (self.out_features,), x.device)\n",
    "        \n",
    "        weight_sigma = F.softplus(self.weight_rho)\n",
    "        bias_sigma = F.softplus(self.bias_rho)\n",
    "        \n",
    "        mean_output = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        \n",
    "        # Compute perturbation with corrected dimensions\n",
    "        r_w = torch.randn_like(self.weight_mu)\n",
    "        perturbed_weights = (r_w * weight_sigma).unsqueeze(0)\n",
    "        \n",
    "        x_reshape = x * self.input_sign\n",
    "        perturbation = F.linear(x_reshape, perturbed_weights.squeeze(0)) * self.output_sign\n",
    "        \n",
    "        bias_perturbation = bias_sigma * torch.randn_like(self.bias_mu)\n",
    "        \n",
    "        return mean_output + perturbation + bias_perturbation\n",
    "\n",
    "    def kl_loss(self):\n",
    "        weight_sigma = F.softplus(self.weight_rho)\n",
    "        bias_sigma = F.softplus(self.bias_rho)\n",
    "        \n",
    "        kl_weight = 0.5 * torch.sum(\n",
    "            torch.log1p((weight_sigma**2) / (self.prior_sigma**2)) +\n",
    "            (self.weight_mu**2) / (self.prior_sigma**2) - 1\n",
    "        )\n",
    "        \n",
    "        kl_bias = 0.5 * torch.sum(\n",
    "            torch.log1p((bias_sigma**2) / (self.prior_sigma**2)) +\n",
    "            (self.bias_mu**2) / (self.prior_sigma**2) - 1\n",
    "        )\n",
    "        \n",
    "        return (kl_weight + kl_bias) / self.weight_mu.numel()\n",
    "\n",
    "class BayesianNNFlipout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = FlipoutLinear(64 * 4 * 4, 512, prior_sigma=0.1)\n",
    "        self.fc2 = FlipoutLinear(512, 10, prior_sigma=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def kl_loss(self):\n",
    "        return self.fc1.kl_loss() + self.fc2.kl_loss()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, epoch, total_epochs=100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    beta = min(1.0, (epoch / (total_epochs * 0.2))) * 0.1\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        nll_loss = F.cross_entropy(output, target)\n",
    "        kl_loss = model.kl_loss()\n",
    "        loss = nll_loss + beta * kl_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Batch: {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'Acc: {100. * correct/total:.2f}%')\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {100. * accuracy:.2f}%')\n",
    "    \n",
    "    return test_loss, accuracy\n",
    "\n",
    "# %%\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "testset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Training and evaluation for different sample sizes\n",
    "sample_sizes = [1, 5, 10, 50, 100, 2000, 4000]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# %%\n",
    "for size in sample_sizes:\n",
    "    hp_size = size\n",
    "\n",
    "    hp_path = f\"../src/results/VI_HP_size_{hp_size}/best_params_sample_{hp_size}.txt\"\n",
    "\n",
    "    # Open the file and extract hyperparameters\n",
    "    with open(hp_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Parse the hyperparameters\n",
    "    batch_size = int(lines[2].split(\":\")[1].strip().strip(\",\"))\n",
    "    prior_sigma = int(lines[3].split(\":\")[1].strip().strip(\",\"))\n",
    "    lr = int(lines[4].split(\":\")[1].strip().strip(\",\"))\n",
    "    epochs = int(lines[5].split(\":\")[1].strip().strip(\",\"))\n",
    "    num_layers = float(lines[6].split(\":\")[1].strip().strip(\",\"))\n",
    "    best_trial = int(lines[10].split(\":\")[1].strip())\n",
    "\n",
    "    print(f\"Best hyperparameters for {size} samples: {batch_size}, {prior_sigma}, {lr}, {num_layers}, {epochs}, {best_trial}\")\n",
    "\n",
    "    hyperparameters = {\n",
    "        'lr': lr, \n",
    "        'num_epochs': epochs, \n",
    "        'batch_size': batch_size, \n",
    "        'prior_sigma': prior_sigma, \n",
    "        'num_layers': num_layers, \n",
    "    }\n",
    "\n",
    "    print(f\"\\Loading model with {size} samples per class...\")\n",
    "    \n",
    "    # Subset dataset to include only 'size' samples per class\n",
    "    indices = []\n",
    "    class_counts = {i: 0 for i in range(10)}\n",
    "    for idx, (_, label) in enumerate(trainset):\n",
    "        if class_counts[label] < size:\n",
    "            indices.append(idx)\n",
    "            class_counts[label] += 1\n",
    "        if all(count >= size for count in class_counts.values()):\n",
    "            break\n",
    "\n",
    "    subset = Subset(trainset, indices)\n",
    "    trainloader = DataLoader(subset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = BayesianNNFlipout().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(f\"../src/models/svhn_model_{size}_samples_VI.pth\"))\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_entropies = []\n",
    "    all_max_probs = []\n",
    "    all_uncertainties = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            mean_output, uncertainty, entropy, max_probs = VI_dropout_predict_with_entropy(model, inputs, n_samples=10)\n",
    "            _, predicted = torch.max(mean_output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_entropies.extend(entropy.cpu().numpy())  # Collect entropies\n",
    "            all_max_probs.extend(max_probs.cpu().numpy())  # Collect max probabilities\n",
    "            all_uncertainties.append(uncertainty.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Calculate statistics for entropy and max probabilities\n",
    "    mean_entropy = np.mean(all_entropies)\n",
    "    std_entropy = np.std(all_entropies)\n",
    "    mean_max_prob = np.mean(all_max_probs)\n",
    "    std_max_prob = np.std(all_max_probs)\n",
    "    print(f\"Mean Entropy: {mean_entropy:.4f}\")\n",
    "    print(f\"Standard Deviation of Entropy: {std_entropy:.4f}\")\n",
    "    print(f\"Mean Max Probability: {mean_max_prob:.4f}\")\n",
    "    print(f\"Standard Deviation of Max Probability: {std_max_prob:.4f}\")\n",
    "\n",
    "    # Save the results\n",
    "    results_path = f\"./results/VI/svhn_results_{size}_samples_VI.json\"\n",
    "    pathlib.Path(\"./results/VI/\").mkdir(parents=True, exist_ok=True)\n",
    "    with open(results_path, \"w\") as f:\n",
    "            json.dump({\"mean_entropy\": f\"{mean_entropy:.4f}\", \"std_entropy\": f\"{std_entropy:.4f}\", \n",
    "                       \"mean_max_prob\": f\"{mean_max_prob:.4f}\", \"std_max_prob\": f\"{std_max_prob:.4f}\"} , f)\n",
    "    print(f\"Results saved at {results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
