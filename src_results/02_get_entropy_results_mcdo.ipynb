{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      "Best hyperparameters for 1 samples: 64, 128, 16, 3, 0.4785845541293291, 0.00015139826171439673, 67\n",
      "\\Loading model with 1 samples per class...\n",
      "Mean Entropy: 2.3014\n",
      "Standard Deviation of Entropy: 0.0001\n",
      "Mean Max Probability: 0.1063\n",
      "Standard Deviation of Max Probability: 0.0002\n",
      "Results saved at ./results/mcdo/svhn_results_1_samples_mcdo.json\n",
      "Best hyperparameters for 5 samples: 128, 128, 32, 4, 0.3226229184566036, 0.00010999461611456229, 78\n",
      "\\Loading model with 5 samples per class...\n",
      "Mean Entropy: 2.3007\n",
      "Standard Deviation of Entropy: 0.0006\n",
      "Mean Max Probability: 0.1100\n",
      "Standard Deviation of Max Probability: 0.0018\n",
      "Results saved at ./results/mcdo/svhn_results_5_samples_mcdo.json\n",
      "Best hyperparameters for 10 samples: 64, 64, 64, 4, 0.28965012336117485, 0.009676006170981157, 97\n",
      "\\Loading model with 10 samples per class...\n",
      "Mean Entropy: 2.3024\n",
      "Standard Deviation of Entropy: 0.0000\n",
      "Mean Max Probability: 0.1036\n",
      "Standard Deviation of Max Probability: 0.0000\n",
      "Results saved at ./results/mcdo/svhn_results_10_samples_mcdo.json\n",
      "Best hyperparameters for 50 samples: 32, 256, 16, 4, 0.2056647830147026, 0.00025234424339213733, 100\n",
      "\\Loading model with 50 samples per class...\n",
      "Mean Entropy: 2.1612\n",
      "Standard Deviation of Entropy: 0.1552\n",
      "Mean Max Probability: 0.2058\n",
      "Standard Deviation of Max Probability: 0.0838\n",
      "Results saved at ./results/mcdo/svhn_results_50_samples_mcdo.json\n",
      "Best hyperparameters for 100 samples: 32, 256, 16, 4, 0.2056647830147026, 0.00025234424339213733, 100\n",
      "\\Loading model with 100 samples per class...\n",
      "Mean Entropy: 1.8182\n",
      "Standard Deviation of Entropy: 0.4237\n",
      "Mean Max Probability: 0.3700\n",
      "Standard Deviation of Max Probability: 0.1851\n",
      "Results saved at ./results/mcdo/svhn_results_100_samples_mcdo.json\n",
      "Best hyperparameters for 2000 samples: 128, 256, 64, 3, 0.21438513203962067, 0.0002638634961365127, 71\n",
      "\\Loading model with 2000 samples per class...\n",
      "Mean Entropy: 0.4322\n",
      "Standard Deviation of Entropy: 0.5274\n",
      "Mean Max Probability: 0.8619\n",
      "Standard Deviation of Max Probability: 0.1963\n",
      "Results saved at ./results/mcdo/svhn_results_2000_samples_mcdo.json\n",
      "Best hyperparameters for 4000 samples: 32, 256, 16, 4, 0.2056647830147026, 0.00025234424339213733, 100\n",
      "\\Loading model with 4000 samples per class...\n",
      "Mean Entropy: 0.5672\n",
      "Standard Deviation of Entropy: 0.5673\n",
      "Mean Max Probability: 0.8324\n",
      "Standard Deviation of Max Probability: 0.2046\n",
      "Results saved at ./results/mcdo/svhn_results_4000_samples_mcdo.json\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pathlib\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "# Setting the seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Evaluate the model with MCDO and calculate entropy\n",
    "def mc_dropout_predict_with_entropy(model, inputs, n_samples=10):\n",
    "    # model.train()  # Enable dropout during prediction\n",
    "    outputs = torch.stack([model(inputs) for _ in range(n_samples)])\n",
    "    mean_output = outputs.mean(dim=0)\n",
    "    uncertainty = outputs.var(dim=0)\n",
    "    # Calculate entropy\n",
    "    probs = F.softmax(mean_output, dim=1)  # Convert logits to probabilities\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=1)  # Add epsilon to avoid log(0)\n",
    "    # Getting the max probability\n",
    "    max_probs, _ = torch.max(probs, dim=1)\n",
    "    return mean_output, uncertainty, entropy, max_probs\n",
    "\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, kernel_size=3, num_neurons=128, conv_neurons=32, num_layers=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Conv2d(3, conv_neurons, kernel_size=kernel_size, stride=1, padding=1))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Intermediate layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Conv2d(conv_neurons, conv_neurons * 2, kernel_size=kernel_size, stride=1, padding=1))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            conv_neurons *= 2\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flattened_size = self._get_flattened_size(3, 32, kernel_size, num_layers)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, num_neurons)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(num_neurons, 10)\n",
    "\n",
    "    def _get_flattened_size(self, input_channels, input_dim, kernel_size, num_layers):\n",
    "        \"\"\"Dynamically calculate the flattened size after conv and pooling layers.\"\"\"\n",
    "        x = torch.zeros((1, input_channels, input_dim, input_dim))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the MCDO wrapper for uncertainty estimation\n",
    "class SimpleCNNWithMCDO(nn.Module):\n",
    "    def __init__(self, base_model, drop_out=0.5):\n",
    "        super(SimpleCNNWithMCDO, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.base_model.layers:\n",
    "            x = self.dropout(layer(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(self.base_model.relu3(self.base_model.fc1(x)))\n",
    "        x = self.base_model.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "testset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Training and evaluation for different sample sizes\n",
    "sample_sizes = [1, 5, 10, 50, 100, 2000, 4000]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# %%\n",
    "for size in sample_sizes:\n",
    "    if size == 100 or size == 50:\n",
    "        hp_size = 4000\n",
    "    else:\n",
    "        hp_size = size\n",
    "\n",
    "    hp_path = f\"../src/results/MCDO_HP_size_{hp_size}/best_params_sample_{hp_size}.txt\"\n",
    "\n",
    "    # Open the file and extract hyperparameters\n",
    "    with open(hp_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Parse the hyperparameters\n",
    "    batch_size = int(lines[2].split(\":\")[1].strip().strip(\",\"))\n",
    "    num_neurons = int(lines[3].split(\":\")[1].strip().strip(\",\"))\n",
    "    conv_neurons = int(lines[4].split(\":\")[1].strip().strip(\",\"))\n",
    "    num_layers = int(lines[5].split(\":\")[1].strip().strip(\",\"))\n",
    "    drop_out = float(lines[6].split(\":\")[1].strip().strip(\",\"))\n",
    "    lr = float(lines[7].split(\":\")[1].strip().strip(\",\"))\n",
    "    num_epochs = int(lines[8].split(\":\")[1].strip().strip(\",\"))\n",
    "    best_trial = int(lines[10].split(\":\")[1].strip())\n",
    "\n",
    "    print(f\"Best hyperparameters for {size} samples: {batch_size}, {num_neurons}, {conv_neurons}, {num_layers}, {drop_out}, {lr}, {num_epochs}\")\n",
    "\n",
    "    hyperparameters = {\n",
    "        'lr': lr, \n",
    "        'drop_out': drop_out, \n",
    "        'num_epochs': num_epochs, \n",
    "        'batch_size': batch_size, \n",
    "        'num_neurons': num_neurons, \n",
    "        'conv_neurons': conv_neurons, \n",
    "        'num_layers': num_layers, \n",
    "    }\n",
    "\n",
    "    print(f\"\\Loading model with {size} samples per class...\")\n",
    "    \n",
    "    # Subset dataset to include only 'size' samples per class\n",
    "    indices = []\n",
    "    class_counts = {i: 0 for i in range(10)}\n",
    "    for idx, (_, label) in enumerate(trainset):\n",
    "        if class_counts[label] < size:\n",
    "            indices.append(idx)\n",
    "            class_counts[label] += 1\n",
    "        if all(count >= size for count in class_counts.values()):\n",
    "            break\n",
    "\n",
    "    subset = Subset(trainset, indices)\n",
    "    trainloader = DataLoader(subset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    base_model = SimpleCNN(kernel_size=3, num_neurons=hyperparameters['num_neurons'], conv_neurons=hyperparameters['conv_neurons'], num_layers=hyperparameters['num_layers']).to(device) \n",
    "    model = SimpleCNNWithMCDO(base_model, drop_out=hyperparameters['drop_out']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(base_model.parameters(), lr=hyperparameters['lr'])\n",
    "\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(f\"../src/models/svhn_model_{size}_samples_mcdo.pth\"))\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_entropies = []\n",
    "    all_max_probs = []\n",
    "    all_uncertainties = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            mean_output, uncertainty, entropy, max_probs = mc_dropout_predict_with_entropy(model, inputs, n_samples=10)\n",
    "            _, predicted = torch.max(mean_output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_entropies.extend(entropy.cpu().numpy())  # Collect entropies\n",
    "            all_max_probs.extend(max_probs.cpu().numpy())  # Collect max probabilities\n",
    "            all_uncertainties.append(uncertainty.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Calculate statistics for entropy and max probabilities\n",
    "    mean_entropy = np.mean(all_entropies)\n",
    "    std_entropy = np.std(all_entropies)\n",
    "    mean_max_prob = np.mean(all_max_probs)\n",
    "    std_max_prob = np.std(all_max_probs)\n",
    "    print(f\"Mean Entropy: {mean_entropy:.4f}\")\n",
    "    print(f\"Standard Deviation of Entropy: {std_entropy:.4f}\")\n",
    "    print(f\"Mean Max Probability: {mean_max_prob:.4f}\")\n",
    "    print(f\"Standard Deviation of Max Probability: {std_max_prob:.4f}\")\n",
    "\n",
    "    # Save the results\n",
    "    results_path = f\"./results/mcdo/svhn_results_{size}_samples_mcdo.json\"\n",
    "    pathlib.Path(\"./results/mcdo/\").mkdir(parents=True, exist_ok=True)\n",
    "    with open(results_path, \"w\") as f:\n",
    "            json.dump({\"mean_entropy\": f\"{mean_entropy:.4f}\", \"std_entropy\": f\"{std_entropy:.4f}\", \n",
    "                       \"mean_max_prob\": f\"{mean_max_prob:.4f}\", \"std_max_prob\": f\"{std_max_prob:.4f}\"} , f)\n",
    "    print(f\"Results saved at {results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
