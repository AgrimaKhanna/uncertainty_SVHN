{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      "Train Batch: 0/573, Loss: 2.3040, Acc: 10.16%\n",
      "Train Batch: 100/573, Loss: 1.6860, Acc: 22.92%\n",
      "Train Batch: 200/573, Loss: 0.9147, Acc: 40.51%\n",
      "Train Batch: 300/573, Loss: 0.6271, Acc: 52.50%\n",
      "Train Batch: 400/573, Loss: 0.5617, Acc: 59.24%\n",
      "Train Batch: 500/573, Loss: 0.6370, Acc: 63.78%\n",
      "Test set: Average loss: 0.4827, Accuracy: 85.91%\n",
      "Epoch: 1/100\n",
      "Train Loss: 1.0164, Train Acc: 66.17%\n",
      "Test Loss: 0.4827, Test Acc: 85.91%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4961, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.4564, Acc: 84.70%\n",
      "Train Batch: 200/573, Loss: 0.3535, Acc: 85.05%\n",
      "Train Batch: 300/573, Loss: 0.5641, Acc: 85.34%\n",
      "Train Batch: 400/573, Loss: 0.3676, Acc: 85.44%\n",
      "Train Batch: 500/573, Loss: 0.4007, Acc: 85.55%\n",
      "Test set: Average loss: 0.3898, Accuracy: 88.44%\n",
      "Epoch: 2/100\n",
      "Train Loss: 0.4702, Train Acc: 85.69%\n",
      "Test Loss: 0.3898, Test Acc: 88.44%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4334, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.3543, Acc: 87.60%\n",
      "Train Batch: 200/573, Loss: 0.6385, Acc: 87.69%\n",
      "Train Batch: 300/573, Loss: 0.2744, Acc: 87.66%\n",
      "Train Batch: 400/573, Loss: 0.4202, Acc: 87.81%\n",
      "Train Batch: 500/573, Loss: 0.2681, Acc: 87.81%\n",
      "Test set: Average loss: 0.3846, Accuracy: 89.10%\n",
      "Epoch: 3/100\n",
      "Train Loss: 0.3967, Train Acc: 87.86%\n",
      "Test Loss: 0.3846, Test Acc: 89.10%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2944, Acc: 92.97%\n",
      "Train Batch: 100/573, Loss: 0.4971, Acc: 89.12%\n",
      "Train Batch: 200/573, Loss: 0.2555, Acc: 88.67%\n",
      "Train Batch: 300/573, Loss: 0.3211, Acc: 88.59%\n",
      "Train Batch: 400/573, Loss: 0.4916, Acc: 88.63%\n",
      "Train Batch: 500/573, Loss: 0.5590, Acc: 88.61%\n",
      "Test set: Average loss: 0.3441, Accuracy: 90.01%\n",
      "Epoch: 4/100\n",
      "Train Loss: 0.3747, Train Acc: 88.56%\n",
      "Test Loss: 0.3441, Test Acc: 90.01%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3666, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.3209, Acc: 88.51%\n",
      "Train Batch: 200/573, Loss: 0.3227, Acc: 88.95%\n",
      "Train Batch: 300/573, Loss: 0.4492, Acc: 88.73%\n",
      "Train Batch: 400/573, Loss: 0.3580, Acc: 88.60%\n",
      "Train Batch: 500/573, Loss: 0.3481, Acc: 88.44%\n",
      "Test set: Average loss: 0.3806, Accuracy: 88.85%\n",
      "Epoch: 5/100\n",
      "Train Loss: 0.3783, Train Acc: 88.37%\n",
      "Test Loss: 0.3806, Test Acc: 88.85%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4068, Acc: 85.16%\n",
      "Train Batch: 100/573, Loss: 0.2999, Acc: 88.52%\n",
      "Train Batch: 200/573, Loss: 0.3919, Acc: 88.18%\n",
      "Train Batch: 300/573, Loss: 0.3046, Acc: 87.95%\n",
      "Train Batch: 400/573, Loss: 0.3210, Acc: 87.68%\n",
      "Train Batch: 500/573, Loss: 0.2220, Acc: 87.55%\n",
      "Test set: Average loss: 0.3895, Accuracy: 88.57%\n",
      "Epoch: 6/100\n",
      "Train Loss: 0.4046, Train Acc: 87.52%\n",
      "Test Loss: 0.3895, Test Acc: 88.57%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5381, Acc: 84.38%\n",
      "Train Batch: 100/573, Loss: 0.3632, Acc: 87.23%\n",
      "Train Batch: 200/573, Loss: 0.5984, Acc: 87.15%\n",
      "Train Batch: 300/573, Loss: 0.4797, Acc: 86.99%\n",
      "Train Batch: 400/573, Loss: 0.3995, Acc: 86.84%\n",
      "Train Batch: 500/573, Loss: 0.4235, Acc: 86.78%\n",
      "Test set: Average loss: 0.3938, Accuracy: 88.52%\n",
      "Epoch: 7/100\n",
      "Train Loss: 0.4410, Train Acc: 86.68%\n",
      "Test Loss: 0.3938, Test Acc: 88.52%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4772, Acc: 83.59%\n",
      "Train Batch: 100/573, Loss: 0.6331, Acc: 86.01%\n",
      "Train Batch: 200/573, Loss: 0.3031, Acc: 85.93%\n",
      "Train Batch: 300/573, Loss: 0.4085, Acc: 85.74%\n",
      "Train Batch: 400/573, Loss: 0.5360, Acc: 85.68%\n",
      "Train Batch: 500/573, Loss: 0.3035, Acc: 85.54%\n",
      "Test set: Average loss: 0.4140, Accuracy: 87.85%\n",
      "Epoch: 8/100\n",
      "Train Loss: 0.4771, Train Acc: 85.50%\n",
      "Test Loss: 0.4140, Test Acc: 87.85%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5273, Acc: 89.06%\n",
      "Train Batch: 100/573, Loss: 0.5741, Acc: 85.00%\n",
      "Train Batch: 200/573, Loss: 0.5357, Acc: 84.90%\n",
      "Train Batch: 300/573, Loss: 0.6288, Acc: 84.89%\n",
      "Train Batch: 400/573, Loss: 0.4363, Acc: 84.94%\n",
      "Train Batch: 500/573, Loss: 0.4641, Acc: 84.81%\n",
      "Test set: Average loss: 0.4615, Accuracy: 86.49%\n",
      "Epoch: 9/100\n",
      "Train Loss: 0.5055, Train Acc: 84.71%\n",
      "Test Loss: 0.4615, Test Acc: 86.49%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5990, Acc: 78.12%\n",
      "Train Batch: 100/573, Loss: 0.4803, Acc: 84.02%\n",
      "Train Batch: 200/573, Loss: 0.6057, Acc: 84.17%\n",
      "Train Batch: 300/573, Loss: 0.4970, Acc: 84.25%\n",
      "Train Batch: 400/573, Loss: 0.5127, Acc: 84.13%\n",
      "Train Batch: 500/573, Loss: 0.5502, Acc: 84.08%\n",
      "Test set: Average loss: 0.4427, Accuracy: 87.02%\n",
      "Epoch: 10/100\n",
      "Train Loss: 0.5294, Train Acc: 84.14%\n",
      "Test Loss: 0.4427, Test Acc: 87.02%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5624, Acc: 82.03%\n",
      "Train Batch: 100/573, Loss: 0.5465, Acc: 84.52%\n",
      "Train Batch: 200/573, Loss: 0.4309, Acc: 84.86%\n",
      "Train Batch: 300/573, Loss: 0.6760, Acc: 85.03%\n",
      "Train Batch: 400/573, Loss: 0.5999, Acc: 85.05%\n",
      "Train Batch: 500/573, Loss: 0.4680, Acc: 85.15%\n",
      "Test set: Average loss: 0.4008, Accuracy: 88.23%\n",
      "Epoch: 11/100\n",
      "Train Loss: 0.4994, Train Acc: 85.12%\n",
      "Test Loss: 0.4008, Test Acc: 88.23%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3838, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.4992, Acc: 85.98%\n",
      "Train Batch: 200/573, Loss: 0.4649, Acc: 85.63%\n",
      "Train Batch: 300/573, Loss: 0.4530, Acc: 85.44%\n",
      "Train Batch: 400/573, Loss: 0.7933, Acc: 85.46%\n",
      "Train Batch: 500/573, Loss: 0.2870, Acc: 85.54%\n",
      "Test set: Average loss: 0.4115, Accuracy: 88.06%\n",
      "Epoch: 12/100\n",
      "Train Loss: 0.4858, Train Acc: 85.53%\n",
      "Test Loss: 0.4115, Test Acc: 88.06%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5069, Acc: 82.81%\n",
      "Train Batch: 100/573, Loss: 0.3888, Acc: 85.73%\n",
      "Train Batch: 200/573, Loss: 0.3605, Acc: 85.94%\n",
      "Train Batch: 300/573, Loss: 0.5314, Acc: 85.96%\n",
      "Train Batch: 400/573, Loss: 0.4209, Acc: 85.91%\n",
      "Train Batch: 500/573, Loss: 0.5485, Acc: 85.71%\n",
      "Test set: Average loss: 0.3957, Accuracy: 88.03%\n",
      "Epoch: 13/100\n",
      "Train Loss: 0.4920, Train Acc: 85.57%\n",
      "Test Loss: 0.3957, Test Acc: 88.03%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5218, Acc: 84.38%\n",
      "Train Batch: 100/573, Loss: 0.4956, Acc: 85.78%\n",
      "Train Batch: 200/573, Loss: 0.5795, Acc: 85.49%\n",
      "Train Batch: 300/573, Loss: 0.4126, Acc: 85.67%\n",
      "Train Batch: 400/573, Loss: 0.6577, Acc: 85.63%\n",
      "Train Batch: 500/573, Loss: 0.6197, Acc: 85.58%\n",
      "Test set: Average loss: 0.4021, Accuracy: 88.10%\n",
      "Epoch: 14/100\n",
      "Train Loss: 0.4886, Train Acc: 85.56%\n",
      "Test Loss: 0.4021, Test Acc: 88.10%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4666, Acc: 85.94%\n",
      "Train Batch: 100/573, Loss: 0.4277, Acc: 86.22%\n",
      "Train Batch: 200/573, Loss: 0.5711, Acc: 85.98%\n",
      "Train Batch: 300/573, Loss: 0.4068, Acc: 85.90%\n",
      "Train Batch: 400/573, Loss: 0.3686, Acc: 85.91%\n",
      "Train Batch: 500/573, Loss: 0.5929, Acc: 85.74%\n",
      "Test set: Average loss: 0.4022, Accuracy: 88.05%\n",
      "Epoch: 15/100\n",
      "Train Loss: 0.4842, Train Acc: 85.69%\n",
      "Test Loss: 0.4022, Test Acc: 88.05%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3347, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.6020, Acc: 86.03%\n",
      "Train Batch: 200/573, Loss: 0.5056, Acc: 85.81%\n",
      "Train Batch: 300/573, Loss: 0.3391, Acc: 85.76%\n",
      "Train Batch: 400/573, Loss: 0.4874, Acc: 85.80%\n",
      "Train Batch: 500/573, Loss: 0.5856, Acc: 85.76%\n",
      "Test set: Average loss: 0.4090, Accuracy: 88.07%\n",
      "Epoch: 16/100\n",
      "Train Loss: 0.4892, Train Acc: 85.68%\n",
      "Test Loss: 0.4090, Test Acc: 88.07%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5439, Acc: 83.59%\n",
      "Train Batch: 100/573, Loss: 0.4611, Acc: 86.45%\n",
      "Train Batch: 200/573, Loss: 0.6168, Acc: 86.48%\n",
      "Train Batch: 300/573, Loss: 0.3458, Acc: 86.41%\n",
      "Train Batch: 400/573, Loss: 0.3601, Acc: 86.53%\n",
      "Train Batch: 500/573, Loss: 0.6072, Acc: 86.60%\n",
      "Test set: Average loss: 0.3845, Accuracy: 88.87%\n",
      "Epoch: 17/100\n",
      "Train Loss: 0.4561, Train Acc: 86.64%\n",
      "Test Loss: 0.3845, Test Acc: 88.87%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4992, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.5009, Acc: 86.91%\n",
      "Train Batch: 200/573, Loss: 0.5363, Acc: 86.45%\n",
      "Train Batch: 300/573, Loss: 0.3373, Acc: 86.66%\n",
      "Train Batch: 400/573, Loss: 0.5530, Acc: 86.67%\n",
      "Train Batch: 500/573, Loss: 0.5901, Acc: 86.65%\n",
      "Test set: Average loss: 0.3855, Accuracy: 88.60%\n",
      "Epoch: 18/100\n",
      "Train Loss: 0.4546, Train Acc: 86.69%\n",
      "Test Loss: 0.3855, Test Acc: 88.60%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4488, Acc: 83.59%\n",
      "Train Batch: 100/573, Loss: 0.3566, Acc: 86.62%\n",
      "Train Batch: 200/573, Loss: 0.4104, Acc: 86.85%\n",
      "Train Batch: 300/573, Loss: 0.4722, Acc: 86.79%\n",
      "Train Batch: 400/573, Loss: 0.4739, Acc: 86.81%\n",
      "Train Batch: 500/573, Loss: 0.3477, Acc: 86.80%\n",
      "Test set: Average loss: 0.3786, Accuracy: 88.91%\n",
      "Epoch: 19/100\n",
      "Train Loss: 0.4482, Train Acc: 86.85%\n",
      "Test Loss: 0.3786, Test Acc: 88.91%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5529, Acc: 83.59%\n",
      "Train Batch: 100/573, Loss: 0.4024, Acc: 87.24%\n",
      "Train Batch: 200/573, Loss: 0.5964, Acc: 86.86%\n",
      "Train Batch: 300/573, Loss: 0.3905, Acc: 86.95%\n",
      "Train Batch: 400/573, Loss: 0.5220, Acc: 86.93%\n",
      "Train Batch: 500/573, Loss: 0.3641, Acc: 86.99%\n",
      "Test set: Average loss: 0.3815, Accuracy: 88.73%\n",
      "Epoch: 20/100\n",
      "Train Loss: 0.4467, Train Acc: 87.00%\n",
      "Test Loss: 0.3815, Test Acc: 88.73%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4254, Acc: 85.16%\n",
      "Train Batch: 100/573, Loss: 0.3806, Acc: 87.62%\n",
      "Train Batch: 200/573, Loss: 0.6009, Acc: 87.18%\n",
      "Train Batch: 300/573, Loss: 0.3631, Acc: 87.14%\n",
      "Train Batch: 400/573, Loss: 0.4350, Acc: 87.10%\n",
      "Train Batch: 500/573, Loss: 0.4840, Acc: 87.17%\n",
      "Test set: Average loss: 0.3779, Accuracy: 88.72%\n",
      "Epoch: 21/100\n",
      "Train Loss: 0.4432, Train Acc: 87.15%\n",
      "Test Loss: 0.3779, Test Acc: 88.72%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4094, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.3756, Acc: 87.13%\n",
      "Train Batch: 200/573, Loss: 0.5120, Acc: 86.95%\n",
      "Train Batch: 300/573, Loss: 0.3966, Acc: 87.11%\n",
      "Train Batch: 400/573, Loss: 0.5138, Acc: 87.04%\n",
      "Train Batch: 500/573, Loss: 0.4852, Acc: 87.11%\n",
      "Test set: Average loss: 0.3757, Accuracy: 89.05%\n",
      "Epoch: 22/100\n",
      "Train Loss: 0.4408, Train Acc: 87.09%\n",
      "Test Loss: 0.3757, Test Acc: 89.05%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4354, Acc: 88.28%\n",
      "Train Batch: 100/573, Loss: 0.3827, Acc: 87.39%\n",
      "Train Batch: 200/573, Loss: 0.4988, Acc: 87.41%\n",
      "Train Batch: 300/573, Loss: 0.6806, Acc: 87.51%\n",
      "Train Batch: 400/573, Loss: 0.3127, Acc: 87.52%\n",
      "Train Batch: 500/573, Loss: 0.2998, Acc: 87.60%\n",
      "Test set: Average loss: 0.3645, Accuracy: 89.24%\n",
      "Epoch: 23/100\n",
      "Train Loss: 0.4268, Train Acc: 87.60%\n",
      "Test Loss: 0.3645, Test Acc: 89.24%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3208, Acc: 91.41%\n",
      "Train Batch: 100/573, Loss: 0.6879, Acc: 88.11%\n",
      "Train Batch: 200/573, Loss: 0.4815, Acc: 87.88%\n",
      "Train Batch: 300/573, Loss: 0.3909, Acc: 87.79%\n",
      "Train Batch: 400/573, Loss: 0.4601, Acc: 87.83%\n",
      "Train Batch: 500/573, Loss: 0.5262, Acc: 87.73%\n",
      "Test set: Average loss: 0.3709, Accuracy: 89.28%\n",
      "Epoch: 24/100\n",
      "Train Loss: 0.4236, Train Acc: 87.71%\n",
      "Test Loss: 0.3709, Test Acc: 89.28%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5748, Acc: 84.38%\n",
      "Train Batch: 100/573, Loss: 0.4472, Acc: 88.03%\n",
      "Train Batch: 200/573, Loss: 0.3902, Acc: 87.98%\n",
      "Train Batch: 300/573, Loss: 0.4591, Acc: 87.95%\n",
      "Train Batch: 400/573, Loss: 0.5367, Acc: 87.86%\n",
      "Train Batch: 500/573, Loss: 0.5069, Acc: 87.82%\n",
      "Test set: Average loss: 0.3602, Accuracy: 89.43%\n",
      "Epoch: 25/100\n",
      "Train Loss: 0.4153, Train Acc: 87.84%\n",
      "Test Loss: 0.3602, Test Acc: 89.43%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2818, Acc: 91.41%\n",
      "Train Batch: 100/573, Loss: 0.4376, Acc: 87.98%\n",
      "Train Batch: 200/573, Loss: 0.4337, Acc: 88.04%\n",
      "Train Batch: 300/573, Loss: 0.3963, Acc: 88.01%\n",
      "Train Batch: 400/573, Loss: 0.3196, Acc: 88.00%\n",
      "Train Batch: 500/573, Loss: 0.6110, Acc: 87.95%\n",
      "Test set: Average loss: 0.3645, Accuracy: 89.56%\n",
      "Epoch: 26/100\n",
      "Train Loss: 0.4151, Train Acc: 87.93%\n",
      "Test Loss: 0.3645, Test Acc: 89.56%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5971, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4081, Acc: 88.24%\n",
      "Train Batch: 200/573, Loss: 0.3203, Acc: 88.13%\n",
      "Train Batch: 300/573, Loss: 0.4458, Acc: 88.11%\n",
      "Train Batch: 400/573, Loss: 0.3818, Acc: 88.12%\n",
      "Train Batch: 500/573, Loss: 0.3712, Acc: 88.04%\n",
      "Test set: Average loss: 0.3618, Accuracy: 89.41%\n",
      "Epoch: 27/100\n",
      "Train Loss: 0.4131, Train Acc: 88.01%\n",
      "Test Loss: 0.3618, Test Acc: 89.41%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3616, Acc: 91.41%\n",
      "Train Batch: 100/573, Loss: 0.5849, Acc: 87.75%\n",
      "Train Batch: 200/573, Loss: 0.4212, Acc: 87.76%\n",
      "Train Batch: 300/573, Loss: 0.4689, Acc: 87.88%\n",
      "Train Batch: 400/573, Loss: 0.3226, Acc: 88.02%\n",
      "Train Batch: 500/573, Loss: 0.4299, Acc: 88.02%\n",
      "Test set: Average loss: 0.3644, Accuracy: 89.43%\n",
      "Epoch: 28/100\n",
      "Train Loss: 0.4167, Train Acc: 87.99%\n",
      "Test Loss: 0.3644, Test Acc: 89.43%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4782, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.4481, Acc: 88.44%\n",
      "Train Batch: 200/573, Loss: 0.4963, Acc: 88.24%\n",
      "Train Batch: 300/573, Loss: 0.4536, Acc: 88.28%\n",
      "Train Batch: 400/573, Loss: 0.5040, Acc: 88.18%\n",
      "Train Batch: 500/573, Loss: 0.2736, Acc: 88.14%\n",
      "Test set: Average loss: 0.3598, Accuracy: 89.64%\n",
      "Epoch: 29/100\n",
      "Train Loss: 0.4141, Train Acc: 88.09%\n",
      "Test Loss: 0.3598, Test Acc: 89.64%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4233, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.4026, Acc: 88.20%\n",
      "Train Batch: 200/573, Loss: 0.4210, Acc: 88.38%\n",
      "Train Batch: 300/573, Loss: 0.6149, Acc: 88.39%\n",
      "Train Batch: 400/573, Loss: 0.3336, Acc: 88.33%\n",
      "Train Batch: 500/573, Loss: 0.3892, Acc: 88.29%\n",
      "Test set: Average loss: 0.3626, Accuracy: 89.59%\n",
      "Epoch: 30/100\n",
      "Train Loss: 0.4060, Train Acc: 88.24%\n",
      "Test Loss: 0.3626, Test Acc: 89.59%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3851, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4098, Acc: 88.36%\n",
      "Train Batch: 200/573, Loss: 0.3649, Acc: 88.46%\n",
      "Train Batch: 300/573, Loss: 0.4381, Acc: 88.25%\n",
      "Train Batch: 400/573, Loss: 0.5759, Acc: 88.24%\n",
      "Train Batch: 500/573, Loss: 0.3982, Acc: 88.20%\n",
      "Test set: Average loss: 0.3620, Accuracy: 89.46%\n",
      "Epoch: 31/100\n",
      "Train Loss: 0.4072, Train Acc: 88.14%\n",
      "Test Loss: 0.3620, Test Acc: 89.46%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3871, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.4637, Acc: 88.02%\n",
      "Train Batch: 200/573, Loss: 0.4322, Acc: 87.97%\n",
      "Train Batch: 300/573, Loss: 0.2640, Acc: 87.92%\n",
      "Train Batch: 400/573, Loss: 0.5107, Acc: 87.96%\n",
      "Train Batch: 500/573, Loss: 0.3990, Acc: 88.11%\n",
      "Test set: Average loss: 0.3631, Accuracy: 89.42%\n",
      "Epoch: 32/100\n",
      "Train Loss: 0.4097, Train Acc: 88.14%\n",
      "Test Loss: 0.3631, Test Acc: 89.42%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3558, Acc: 88.28%\n",
      "Train Batch: 100/573, Loss: 0.3520, Acc: 88.52%\n",
      "Train Batch: 200/573, Loss: 0.3683, Acc: 88.49%\n",
      "Train Batch: 300/573, Loss: 0.6231, Acc: 88.31%\n",
      "Train Batch: 400/573, Loss: 0.5682, Acc: 88.20%\n",
      "Train Batch: 500/573, Loss: 0.2190, Acc: 88.28%\n",
      "Test set: Average loss: 0.3642, Accuracy: 89.64%\n",
      "Epoch: 33/100\n",
      "Train Loss: 0.4042, Train Acc: 88.28%\n",
      "Test Loss: 0.3642, Test Acc: 89.64%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3512, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.2890, Acc: 87.96%\n",
      "Train Batch: 200/573, Loss: 0.3306, Acc: 88.11%\n",
      "Train Batch: 300/573, Loss: 0.4956, Acc: 88.13%\n",
      "Train Batch: 400/573, Loss: 0.4297, Acc: 88.18%\n",
      "Train Batch: 500/573, Loss: 0.3611, Acc: 88.34%\n",
      "Test set: Average loss: 0.3542, Accuracy: 89.80%\n",
      "Epoch: 34/100\n",
      "Train Loss: 0.4010, Train Acc: 88.33%\n",
      "Test Loss: 0.3542, Test Acc: 89.80%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4563, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.4427, Acc: 88.47%\n",
      "Train Batch: 200/573, Loss: 0.3183, Acc: 88.61%\n",
      "Train Batch: 300/573, Loss: 0.3710, Acc: 88.47%\n",
      "Train Batch: 400/573, Loss: 0.4083, Acc: 88.43%\n",
      "Train Batch: 500/573, Loss: 0.4736, Acc: 88.40%\n",
      "Test set: Average loss: 0.3558, Accuracy: 89.77%\n",
      "Epoch: 35/100\n",
      "Train Loss: 0.4004, Train Acc: 88.40%\n",
      "Test Loss: 0.3558, Test Acc: 89.77%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5170, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.3791, Acc: 88.10%\n",
      "Train Batch: 200/573, Loss: 0.3634, Acc: 88.22%\n",
      "Train Batch: 300/573, Loss: 0.2509, Acc: 88.37%\n",
      "Train Batch: 400/573, Loss: 0.3029, Acc: 88.59%\n",
      "Train Batch: 500/573, Loss: 0.2573, Acc: 88.58%\n",
      "Test set: Average loss: 0.3544, Accuracy: 89.75%\n",
      "Epoch: 36/100\n",
      "Train Loss: 0.3939, Train Acc: 88.53%\n",
      "Test Loss: 0.3544, Test Acc: 89.75%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3351, Acc: 88.28%\n",
      "Train Batch: 100/573, Loss: 0.4674, Acc: 88.60%\n",
      "Train Batch: 200/573, Loss: 0.5440, Acc: 88.45%\n",
      "Train Batch: 300/573, Loss: 0.4372, Acc: 88.52%\n",
      "Train Batch: 400/573, Loss: 0.2785, Acc: 88.53%\n",
      "Train Batch: 500/573, Loss: 0.3175, Acc: 88.49%\n",
      "Test set: Average loss: 0.3587, Accuracy: 89.61%\n",
      "Epoch: 37/100\n",
      "Train Loss: 0.3955, Train Acc: 88.50%\n",
      "Test Loss: 0.3587, Test Acc: 89.61%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4584, Acc: 85.94%\n",
      "Train Batch: 100/573, Loss: 0.5170, Acc: 88.44%\n",
      "Train Batch: 200/573, Loss: 0.3451, Acc: 88.64%\n",
      "Train Batch: 300/573, Loss: 0.4077, Acc: 88.44%\n",
      "Train Batch: 400/573, Loss: 0.3895, Acc: 88.50%\n",
      "Train Batch: 500/573, Loss: 0.3976, Acc: 88.47%\n",
      "Test set: Average loss: 0.3569, Accuracy: 89.64%\n",
      "Epoch: 38/100\n",
      "Train Loss: 0.3951, Train Acc: 88.50%\n",
      "Test Loss: 0.3569, Test Acc: 89.64%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4651, Acc: 85.94%\n",
      "Train Batch: 100/573, Loss: 0.2857, Acc: 88.46%\n",
      "Train Batch: 200/573, Loss: 0.2592, Acc: 88.47%\n",
      "Train Batch: 300/573, Loss: 0.5307, Acc: 88.41%\n",
      "Train Batch: 400/573, Loss: 0.3395, Acc: 88.32%\n",
      "Train Batch: 500/573, Loss: 0.3592, Acc: 88.43%\n",
      "Test set: Average loss: 0.3545, Accuracy: 89.82%\n",
      "Epoch: 39/100\n",
      "Train Loss: 0.3990, Train Acc: 88.37%\n",
      "Test Loss: 0.3545, Test Acc: 89.82%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4814, Acc: 84.38%\n",
      "Train Batch: 100/573, Loss: 0.4573, Acc: 88.71%\n",
      "Train Batch: 200/573, Loss: 0.3399, Acc: 88.55%\n",
      "Train Batch: 300/573, Loss: 0.2936, Acc: 88.48%\n",
      "Train Batch: 400/573, Loss: 0.4216, Acc: 88.51%\n",
      "Train Batch: 500/573, Loss: 0.3847, Acc: 88.48%\n",
      "Test set: Average loss: 0.3545, Accuracy: 89.79%\n",
      "Epoch: 40/100\n",
      "Train Loss: 0.3988, Train Acc: 88.46%\n",
      "Test Loss: 0.3545, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3140, Acc: 89.06%\n",
      "Train Batch: 100/573, Loss: 0.2661, Acc: 88.61%\n",
      "Train Batch: 200/573, Loss: 0.2982, Acc: 88.40%\n",
      "Train Batch: 300/573, Loss: 0.2965, Acc: 88.44%\n",
      "Train Batch: 400/573, Loss: 0.1719, Acc: 88.49%\n",
      "Train Batch: 500/573, Loss: 0.3789, Acc: 88.53%\n",
      "Test set: Average loss: 0.3530, Accuracy: 89.82%\n",
      "Epoch: 41/100\n",
      "Train Loss: 0.3939, Train Acc: 88.54%\n",
      "Test Loss: 0.3530, Test Acc: 89.82%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4218, Acc: 88.28%\n",
      "Train Batch: 100/573, Loss: 0.3543, Acc: 88.51%\n",
      "Train Batch: 200/573, Loss: 0.3980, Acc: 88.49%\n",
      "Train Batch: 300/573, Loss: 0.4433, Acc: 88.43%\n",
      "Train Batch: 400/573, Loss: 0.3403, Acc: 88.66%\n",
      "Train Batch: 500/573, Loss: 0.4480, Acc: 88.58%\n",
      "Test set: Average loss: 0.3549, Accuracy: 89.74%\n",
      "Epoch: 42/100\n",
      "Train Loss: 0.3932, Train Acc: 88.59%\n",
      "Test Loss: 0.3549, Test Acc: 89.74%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5545, Acc: 84.38%\n",
      "Train Batch: 100/573, Loss: 0.4026, Acc: 88.50%\n",
      "Train Batch: 200/573, Loss: 0.3900, Acc: 88.41%\n",
      "Train Batch: 300/573, Loss: 0.4341, Acc: 88.38%\n",
      "Train Batch: 400/573, Loss: 0.3862, Acc: 88.52%\n",
      "Train Batch: 500/573, Loss: 0.4236, Acc: 88.49%\n",
      "Test set: Average loss: 0.3532, Accuracy: 89.82%\n",
      "Epoch: 43/100\n",
      "Train Loss: 0.3947, Train Acc: 88.49%\n",
      "Test Loss: 0.3532, Test Acc: 89.82%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4643, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.4613, Acc: 88.64%\n",
      "Train Batch: 200/573, Loss: 0.3586, Acc: 88.50%\n",
      "Train Batch: 300/573, Loss: 0.2534, Acc: 88.58%\n",
      "Train Batch: 400/573, Loss: 0.4449, Acc: 88.68%\n",
      "Train Batch: 500/573, Loss: 0.3025, Acc: 88.70%\n",
      "Test set: Average loss: 0.3541, Accuracy: 89.88%\n",
      "Epoch: 44/100\n",
      "Train Loss: 0.3927, Train Acc: 88.67%\n",
      "Test Loss: 0.3541, Test Acc: 89.88%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3882, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.2744, Acc: 88.51%\n",
      "Train Batch: 200/573, Loss: 0.3273, Acc: 88.72%\n",
      "Train Batch: 300/573, Loss: 0.4228, Acc: 88.55%\n",
      "Train Batch: 400/573, Loss: 0.3500, Acc: 88.45%\n",
      "Train Batch: 500/573, Loss: 0.4102, Acc: 88.52%\n",
      "Test set: Average loss: 0.3554, Accuracy: 90.01%\n",
      "Epoch: 45/100\n",
      "Train Loss: 0.3931, Train Acc: 88.53%\n",
      "Test Loss: 0.3554, Test Acc: 90.01%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3275, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4542, Acc: 88.17%\n",
      "Train Batch: 200/573, Loss: 0.4813, Acc: 88.35%\n",
      "Train Batch: 300/573, Loss: 0.4265, Acc: 88.51%\n",
      "Train Batch: 400/573, Loss: 0.3133, Acc: 88.64%\n",
      "Train Batch: 500/573, Loss: 0.5388, Acc: 88.64%\n",
      "Test set: Average loss: 0.3539, Accuracy: 89.84%\n",
      "Epoch: 46/100\n",
      "Train Loss: 0.3929, Train Acc: 88.55%\n",
      "Test Loss: 0.3539, Test Acc: 89.84%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4389, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.3459, Acc: 88.54%\n",
      "Train Batch: 200/573, Loss: 0.3846, Acc: 88.62%\n",
      "Train Batch: 300/573, Loss: 0.3412, Acc: 88.77%\n",
      "Train Batch: 400/573, Loss: 0.4786, Acc: 88.75%\n",
      "Train Batch: 500/573, Loss: 0.3447, Acc: 88.72%\n",
      "Test set: Average loss: 0.3564, Accuracy: 89.65%\n",
      "Epoch: 47/100\n",
      "Train Loss: 0.3923, Train Acc: 88.66%\n",
      "Test Loss: 0.3564, Test Acc: 89.65%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2787, Acc: 91.41%\n",
      "Train Batch: 100/573, Loss: 0.5122, Acc: 88.69%\n",
      "Train Batch: 200/573, Loss: 0.3787, Acc: 88.87%\n",
      "Train Batch: 300/573, Loss: 0.4030, Acc: 88.74%\n",
      "Train Batch: 400/573, Loss: 0.3302, Acc: 88.68%\n",
      "Train Batch: 500/573, Loss: 0.5434, Acc: 88.70%\n",
      "Test set: Average loss: 0.3569, Accuracy: 89.76%\n",
      "Epoch: 48/100\n",
      "Train Loss: 0.3914, Train Acc: 88.68%\n",
      "Test Loss: 0.3569, Test Acc: 89.76%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3205, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4238, Acc: 88.75%\n",
      "Train Batch: 200/573, Loss: 0.2414, Acc: 88.74%\n",
      "Train Batch: 300/573, Loss: 0.4080, Acc: 88.73%\n",
      "Train Batch: 400/573, Loss: 0.2368, Acc: 88.83%\n",
      "Train Batch: 500/573, Loss: 0.2740, Acc: 88.74%\n",
      "Test set: Average loss: 0.3528, Accuracy: 89.79%\n",
      "Epoch: 49/100\n",
      "Train Loss: 0.3836, Train Acc: 88.71%\n",
      "Test Loss: 0.3528, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4673, Acc: 88.28%\n",
      "Train Batch: 100/573, Loss: 0.5544, Acc: 88.47%\n",
      "Train Batch: 200/573, Loss: 0.3705, Acc: 88.48%\n",
      "Train Batch: 300/573, Loss: 0.5241, Acc: 88.57%\n",
      "Train Batch: 400/573, Loss: 0.3835, Acc: 88.59%\n",
      "Train Batch: 500/573, Loss: 0.3852, Acc: 88.64%\n",
      "Test set: Average loss: 0.3525, Accuracy: 89.97%\n",
      "Epoch: 50/100\n",
      "Train Loss: 0.3904, Train Acc: 88.66%\n",
      "Test Loss: 0.3525, Test Acc: 89.97%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2279, Acc: 92.97%\n",
      "Train Batch: 100/573, Loss: 0.4047, Acc: 88.76%\n",
      "Train Batch: 200/573, Loss: 0.3929, Acc: 88.76%\n",
      "Train Batch: 300/573, Loss: 0.3557, Acc: 88.68%\n",
      "Train Batch: 400/573, Loss: 0.4766, Acc: 88.76%\n",
      "Train Batch: 500/573, Loss: 0.3528, Acc: 88.73%\n",
      "Test set: Average loss: 0.3524, Accuracy: 89.79%\n",
      "Epoch: 51/100\n",
      "Train Loss: 0.3930, Train Acc: 88.68%\n",
      "Test Loss: 0.3524, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3547, Acc: 89.06%\n",
      "Train Batch: 100/573, Loss: 0.4470, Acc: 88.44%\n",
      "Train Batch: 200/573, Loss: 0.4128, Acc: 88.34%\n",
      "Train Batch: 300/573, Loss: 0.4246, Acc: 88.56%\n",
      "Train Batch: 400/573, Loss: 0.4197, Acc: 88.63%\n",
      "Train Batch: 500/573, Loss: 0.4945, Acc: 88.67%\n",
      "Test set: Average loss: 0.3553, Accuracy: 89.91%\n",
      "Epoch: 52/100\n",
      "Train Loss: 0.3883, Train Acc: 88.69%\n",
      "Test Loss: 0.3553, Test Acc: 89.91%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2662, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4961, Acc: 88.64%\n",
      "Train Batch: 200/573, Loss: 0.4423, Acc: 88.60%\n",
      "Train Batch: 300/573, Loss: 0.4728, Acc: 88.51%\n",
      "Train Batch: 400/573, Loss: 0.4471, Acc: 88.57%\n",
      "Train Batch: 500/573, Loss: 0.3162, Acc: 88.58%\n",
      "Test set: Average loss: 0.3497, Accuracy: 89.96%\n",
      "Epoch: 53/100\n",
      "Train Loss: 0.3924, Train Acc: 88.60%\n",
      "Test Loss: 0.3497, Test Acc: 89.96%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3564, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.3687, Acc: 89.16%\n",
      "Train Batch: 200/573, Loss: 0.3053, Acc: 89.13%\n",
      "Train Batch: 300/573, Loss: 0.3390, Acc: 89.08%\n",
      "Train Batch: 400/573, Loss: 0.2963, Acc: 89.02%\n",
      "Train Batch: 500/573, Loss: 0.3738, Acc: 89.01%\n",
      "Test set: Average loss: 0.3499, Accuracy: 89.96%\n",
      "Epoch: 54/100\n",
      "Train Loss: 0.3853, Train Acc: 88.92%\n",
      "Test Loss: 0.3499, Test Acc: 89.96%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5431, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.4205, Acc: 88.20%\n",
      "Train Batch: 200/573, Loss: 0.4679, Acc: 88.43%\n",
      "Train Batch: 300/573, Loss: 0.4484, Acc: 88.53%\n",
      "Train Batch: 400/573, Loss: 0.4698, Acc: 88.63%\n",
      "Train Batch: 500/573, Loss: 0.5136, Acc: 88.64%\n",
      "Test set: Average loss: 0.3576, Accuracy: 89.80%\n",
      "Epoch: 55/100\n",
      "Train Loss: 0.3912, Train Acc: 88.64%\n",
      "Test Loss: 0.3576, Test Acc: 89.80%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3818, Acc: 88.28%\n",
      "Train Batch: 100/573, Loss: 0.4439, Acc: 88.58%\n",
      "Train Batch: 200/573, Loss: 0.2961, Acc: 88.77%\n",
      "Train Batch: 300/573, Loss: 0.3653, Acc: 88.83%\n",
      "Train Batch: 400/573, Loss: 0.4506, Acc: 88.76%\n",
      "Train Batch: 500/573, Loss: 0.5277, Acc: 88.62%\n",
      "Test set: Average loss: 0.3543, Accuracy: 89.82%\n",
      "Epoch: 56/100\n",
      "Train Loss: 0.3909, Train Acc: 88.62%\n",
      "Test Loss: 0.3543, Test Acc: 89.82%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2842, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.5173, Acc: 88.88%\n",
      "Train Batch: 200/573, Loss: 0.4747, Acc: 89.00%\n",
      "Train Batch: 300/573, Loss: 0.2821, Acc: 89.04%\n",
      "Train Batch: 400/573, Loss: 0.3952, Acc: 88.92%\n",
      "Train Batch: 500/573, Loss: 0.3269, Acc: 88.93%\n",
      "Test set: Average loss: 0.3570, Accuracy: 89.75%\n",
      "Epoch: 57/100\n",
      "Train Loss: 0.3847, Train Acc: 88.91%\n",
      "Test Loss: 0.3570, Test Acc: 89.75%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3350, Acc: 89.06%\n",
      "Train Batch: 100/573, Loss: 0.3604, Acc: 88.47%\n",
      "Train Batch: 200/573, Loss: 0.2769, Acc: 88.21%\n",
      "Train Batch: 300/573, Loss: 0.3382, Acc: 88.41%\n",
      "Train Batch: 400/573, Loss: 0.3202, Acc: 88.56%\n",
      "Train Batch: 500/573, Loss: 0.5037, Acc: 88.63%\n",
      "Test set: Average loss: 0.3556, Accuracy: 89.78%\n",
      "Epoch: 58/100\n",
      "Train Loss: 0.3926, Train Acc: 88.58%\n",
      "Test Loss: 0.3556, Test Acc: 89.78%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3736, Acc: 92.19%\n",
      "Train Batch: 100/573, Loss: 0.3252, Acc: 88.79%\n",
      "Train Batch: 200/573, Loss: 0.3237, Acc: 88.61%\n",
      "Train Batch: 300/573, Loss: 0.4332, Acc: 88.57%\n",
      "Train Batch: 400/573, Loss: 0.4063, Acc: 88.59%\n",
      "Train Batch: 500/573, Loss: 0.5036, Acc: 88.75%\n",
      "Test set: Average loss: 0.3570, Accuracy: 89.84%\n",
      "Epoch: 59/100\n",
      "Train Loss: 0.3882, Train Acc: 88.76%\n",
      "Test Loss: 0.3570, Test Acc: 89.84%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3062, Acc: 92.97%\n",
      "Train Batch: 100/573, Loss: 0.4372, Acc: 88.40%\n",
      "Train Batch: 200/573, Loss: 0.3544, Acc: 88.67%\n",
      "Train Batch: 300/573, Loss: 0.3250, Acc: 88.70%\n",
      "Train Batch: 400/573, Loss: 0.3650, Acc: 88.72%\n",
      "Train Batch: 500/573, Loss: 0.4811, Acc: 88.69%\n",
      "Test set: Average loss: 0.3551, Accuracy: 89.79%\n",
      "Epoch: 60/100\n",
      "Train Loss: 0.3883, Train Acc: 88.68%\n",
      "Test Loss: 0.3551, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5048, Acc: 84.38%\n",
      "Train Batch: 100/573, Loss: 0.6191, Acc: 88.73%\n",
      "Train Batch: 200/573, Loss: 0.3680, Acc: 88.69%\n",
      "Train Batch: 300/573, Loss: 0.2424, Acc: 88.58%\n",
      "Train Batch: 400/573, Loss: 0.3638, Acc: 88.47%\n",
      "Train Batch: 500/573, Loss: 0.3794, Acc: 88.53%\n",
      "Test set: Average loss: 0.3480, Accuracy: 89.89%\n",
      "Epoch: 61/100\n",
      "Train Loss: 0.3937, Train Acc: 88.52%\n",
      "Test Loss: 0.3480, Test Acc: 89.89%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2908, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4583, Acc: 88.65%\n",
      "Train Batch: 200/573, Loss: 0.3813, Acc: 88.53%\n",
      "Train Batch: 300/573, Loss: 0.4014, Acc: 88.71%\n",
      "Train Batch: 400/573, Loss: 0.3085, Acc: 88.77%\n",
      "Train Batch: 500/573, Loss: 0.4536, Acc: 88.67%\n",
      "Test set: Average loss: 0.3535, Accuracy: 89.91%\n",
      "Epoch: 62/100\n",
      "Train Loss: 0.3908, Train Acc: 88.70%\n",
      "Test Loss: 0.3535, Test Acc: 89.91%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3955, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.2910, Acc: 88.69%\n",
      "Train Batch: 200/573, Loss: 0.4212, Acc: 88.76%\n",
      "Train Batch: 300/573, Loss: 0.6025, Acc: 88.77%\n",
      "Train Batch: 400/573, Loss: 0.6371, Acc: 88.68%\n",
      "Train Batch: 500/573, Loss: 0.5135, Acc: 88.72%\n",
      "Test set: Average loss: 0.3562, Accuracy: 89.74%\n",
      "Epoch: 63/100\n",
      "Train Loss: 0.3877, Train Acc: 88.74%\n",
      "Test Loss: 0.3562, Test Acc: 89.74%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4362, Acc: 85.94%\n",
      "Train Batch: 100/573, Loss: 0.2759, Acc: 88.70%\n",
      "Train Batch: 200/573, Loss: 0.4697, Acc: 88.70%\n",
      "Train Batch: 300/573, Loss: 0.4407, Acc: 88.79%\n",
      "Train Batch: 400/573, Loss: 0.4939, Acc: 88.89%\n",
      "Train Batch: 500/573, Loss: 0.5525, Acc: 88.88%\n",
      "Test set: Average loss: 0.3515, Accuracy: 90.05%\n",
      "Epoch: 64/100\n",
      "Train Loss: 0.3880, Train Acc: 88.82%\n",
      "Test Loss: 0.3515, Test Acc: 90.05%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4557, Acc: 85.94%\n",
      "Train Batch: 100/573, Loss: 0.2956, Acc: 89.05%\n",
      "Train Batch: 200/573, Loss: 0.3510, Acc: 88.91%\n",
      "Train Batch: 300/573, Loss: 0.3249, Acc: 88.98%\n",
      "Train Batch: 400/573, Loss: 0.3026, Acc: 88.93%\n",
      "Train Batch: 500/573, Loss: 0.5426, Acc: 88.87%\n",
      "Test set: Average loss: 0.3533, Accuracy: 89.75%\n",
      "Epoch: 65/100\n",
      "Train Loss: 0.3824, Train Acc: 88.90%\n",
      "Test Loss: 0.3533, Test Acc: 89.75%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4636, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.4076, Acc: 88.88%\n",
      "Train Batch: 200/573, Loss: 0.3573, Acc: 88.92%\n",
      "Train Batch: 300/573, Loss: 0.4005, Acc: 88.77%\n",
      "Train Batch: 400/573, Loss: 0.3334, Acc: 88.79%\n",
      "Train Batch: 500/573, Loss: 0.2619, Acc: 88.89%\n",
      "Test set: Average loss: 0.3551, Accuracy: 89.61%\n",
      "Epoch: 66/100\n",
      "Train Loss: 0.3869, Train Acc: 88.91%\n",
      "Test Loss: 0.3551, Test Acc: 89.61%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3295, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.2525, Acc: 89.04%\n",
      "Train Batch: 200/573, Loss: 0.3774, Acc: 88.91%\n",
      "Train Batch: 300/573, Loss: 0.3071, Acc: 88.77%\n",
      "Train Batch: 400/573, Loss: 0.4187, Acc: 88.73%\n",
      "Train Batch: 500/573, Loss: 0.3083, Acc: 88.76%\n",
      "Test set: Average loss: 0.3570, Accuracy: 89.90%\n",
      "Epoch: 67/100\n",
      "Train Loss: 0.3883, Train Acc: 88.81%\n",
      "Test Loss: 0.3570, Test Acc: 89.90%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3207, Acc: 91.41%\n",
      "Train Batch: 100/573, Loss: 0.3432, Acc: 88.75%\n",
      "Train Batch: 200/573, Loss: 0.4280, Acc: 88.90%\n",
      "Train Batch: 300/573, Loss: 0.3835, Acc: 88.85%\n",
      "Train Batch: 400/573, Loss: 0.3024, Acc: 88.89%\n",
      "Train Batch: 500/573, Loss: 0.3488, Acc: 88.88%\n",
      "Test set: Average loss: 0.3531, Accuracy: 89.80%\n",
      "Epoch: 68/100\n",
      "Train Loss: 0.3864, Train Acc: 88.82%\n",
      "Test Loss: 0.3531, Test Acc: 89.80%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3052, Acc: 91.41%\n",
      "Train Batch: 100/573, Loss: 0.3830, Acc: 89.12%\n",
      "Train Batch: 200/573, Loss: 0.5675, Acc: 88.68%\n",
      "Train Batch: 300/573, Loss: 0.3022, Acc: 88.65%\n",
      "Train Batch: 400/573, Loss: 0.4486, Acc: 88.67%\n",
      "Train Batch: 500/573, Loss: 0.2804, Acc: 88.74%\n",
      "Test set: Average loss: 0.3527, Accuracy: 89.72%\n",
      "Epoch: 69/100\n",
      "Train Loss: 0.3848, Train Acc: 88.78%\n",
      "Test Loss: 0.3527, Test Acc: 89.72%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3797, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.4093, Acc: 89.00%\n",
      "Train Batch: 200/573, Loss: 0.4452, Acc: 88.96%\n",
      "Train Batch: 300/573, Loss: 0.4182, Acc: 88.89%\n",
      "Train Batch: 400/573, Loss: 0.4103, Acc: 88.74%\n",
      "Train Batch: 500/573, Loss: 0.4044, Acc: 88.71%\n",
      "Test set: Average loss: 0.3504, Accuracy: 89.86%\n",
      "Epoch: 70/100\n",
      "Train Loss: 0.3904, Train Acc: 88.72%\n",
      "Test Loss: 0.3504, Test Acc: 89.86%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2025, Acc: 94.53%\n",
      "Train Batch: 100/573, Loss: 0.3783, Acc: 88.81%\n",
      "Train Batch: 200/573, Loss: 0.4416, Acc: 88.93%\n",
      "Train Batch: 300/573, Loss: 0.4344, Acc: 88.96%\n",
      "Train Batch: 400/573, Loss: 0.2458, Acc: 88.86%\n",
      "Train Batch: 500/573, Loss: 0.4269, Acc: 88.80%\n",
      "Test set: Average loss: 0.3529, Accuracy: 89.89%\n",
      "Epoch: 71/100\n",
      "Train Loss: 0.3903, Train Acc: 88.77%\n",
      "Test Loss: 0.3529, Test Acc: 89.89%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5037, Acc: 88.28%\n",
      "Train Batch: 100/573, Loss: 0.3652, Acc: 89.15%\n",
      "Train Batch: 200/573, Loss: 0.4656, Acc: 88.98%\n",
      "Train Batch: 300/573, Loss: 0.6504, Acc: 88.81%\n",
      "Train Batch: 400/573, Loss: 0.5864, Acc: 88.82%\n",
      "Train Batch: 500/573, Loss: 0.4166, Acc: 88.83%\n",
      "Test set: Average loss: 0.3532, Accuracy: 89.95%\n",
      "Epoch: 72/100\n",
      "Train Loss: 0.3874, Train Acc: 88.79%\n",
      "Test Loss: 0.3532, Test Acc: 89.95%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4418, Acc: 89.06%\n",
      "Train Batch: 100/573, Loss: 0.4446, Acc: 88.77%\n",
      "Train Batch: 200/573, Loss: 0.3835, Acc: 88.82%\n",
      "Train Batch: 300/573, Loss: 0.3482, Acc: 88.92%\n",
      "Train Batch: 400/573, Loss: 0.4502, Acc: 88.87%\n",
      "Train Batch: 500/573, Loss: 0.4329, Acc: 88.83%\n",
      "Test set: Average loss: 0.3526, Accuracy: 89.72%\n",
      "Epoch: 73/100\n",
      "Train Loss: 0.3859, Train Acc: 88.84%\n",
      "Test Loss: 0.3526, Test Acc: 89.72%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4661, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.3937, Acc: 89.02%\n",
      "Train Batch: 200/573, Loss: 0.2314, Acc: 88.79%\n",
      "Train Batch: 300/573, Loss: 0.1931, Acc: 88.55%\n",
      "Train Batch: 400/573, Loss: 0.5034, Acc: 88.53%\n",
      "Train Batch: 500/573, Loss: 0.2869, Acc: 88.65%\n",
      "Test set: Average loss: 0.3609, Accuracy: 89.75%\n",
      "Epoch: 74/100\n",
      "Train Loss: 0.3889, Train Acc: 88.62%\n",
      "Test Loss: 0.3609, Test Acc: 89.75%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5413, Acc: 85.16%\n",
      "Train Batch: 100/573, Loss: 0.3728, Acc: 89.07%\n",
      "Train Batch: 200/573, Loss: 0.2508, Acc: 88.84%\n",
      "Train Batch: 300/573, Loss: 0.6119, Acc: 88.86%\n",
      "Train Batch: 400/573, Loss: 0.3313, Acc: 88.74%\n",
      "Train Batch: 500/573, Loss: 0.3504, Acc: 88.72%\n",
      "Test set: Average loss: 0.3529, Accuracy: 89.77%\n",
      "Epoch: 75/100\n",
      "Train Loss: 0.3887, Train Acc: 88.69%\n",
      "Test Loss: 0.3529, Test Acc: 89.77%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3601, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4279, Acc: 87.99%\n",
      "Train Batch: 200/573, Loss: 0.4970, Acc: 88.53%\n",
      "Train Batch: 300/573, Loss: 0.3967, Acc: 88.71%\n",
      "Train Batch: 400/573, Loss: 0.3329, Acc: 88.71%\n",
      "Train Batch: 500/573, Loss: 0.2839, Acc: 88.75%\n",
      "Test set: Average loss: 0.3521, Accuracy: 89.79%\n",
      "Epoch: 76/100\n",
      "Train Loss: 0.3878, Train Acc: 88.74%\n",
      "Test Loss: 0.3521, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4424, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.3254, Acc: 88.46%\n",
      "Train Batch: 200/573, Loss: 0.6608, Acc: 88.55%\n",
      "Train Batch: 300/573, Loss: 0.3895, Acc: 88.50%\n",
      "Train Batch: 400/573, Loss: 0.2529, Acc: 88.59%\n",
      "Train Batch: 500/573, Loss: 0.2905, Acc: 88.54%\n",
      "Test set: Average loss: 0.3568, Accuracy: 89.58%\n",
      "Epoch: 77/100\n",
      "Train Loss: 0.3900, Train Acc: 88.62%\n",
      "Test Loss: 0.3568, Test Acc: 89.58%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4785, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.4170, Acc: 88.81%\n",
      "Train Batch: 200/573, Loss: 0.2641, Acc: 88.85%\n",
      "Train Batch: 300/573, Loss: 0.3048, Acc: 88.65%\n",
      "Train Batch: 400/573, Loss: 0.3744, Acc: 88.64%\n",
      "Train Batch: 500/573, Loss: 0.5639, Acc: 88.68%\n",
      "Test set: Average loss: 0.3541, Accuracy: 89.96%\n",
      "Epoch: 78/100\n",
      "Train Loss: 0.3886, Train Acc: 88.74%\n",
      "Test Loss: 0.3541, Test Acc: 89.96%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3293, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.3089, Acc: 88.62%\n",
      "Train Batch: 200/573, Loss: 0.3128, Acc: 88.87%\n",
      "Train Batch: 300/573, Loss: 0.2865, Acc: 88.89%\n",
      "Train Batch: 400/573, Loss: 0.5395, Acc: 88.82%\n",
      "Train Batch: 500/573, Loss: 0.3731, Acc: 88.76%\n",
      "Test set: Average loss: 0.3506, Accuracy: 89.92%\n",
      "Epoch: 79/100\n",
      "Train Loss: 0.3875, Train Acc: 88.78%\n",
      "Test Loss: 0.3506, Test Acc: 89.92%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.6334, Acc: 83.59%\n",
      "Train Batch: 100/573, Loss: 0.2952, Acc: 88.37%\n",
      "Train Batch: 200/573, Loss: 0.3396, Acc: 88.66%\n",
      "Train Batch: 300/573, Loss: 0.4836, Acc: 88.55%\n",
      "Train Batch: 400/573, Loss: 0.3838, Acc: 88.70%\n",
      "Train Batch: 500/573, Loss: 0.3511, Acc: 88.66%\n",
      "Test set: Average loss: 0.3506, Accuracy: 89.93%\n",
      "Epoch: 80/100\n",
      "Train Loss: 0.3899, Train Acc: 88.70%\n",
      "Test Loss: 0.3506, Test Acc: 89.93%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2414, Acc: 92.19%\n",
      "Train Batch: 100/573, Loss: 0.3338, Acc: 88.47%\n",
      "Train Batch: 200/573, Loss: 0.3133, Acc: 88.65%\n",
      "Train Batch: 300/573, Loss: 0.3369, Acc: 88.76%\n",
      "Train Batch: 400/573, Loss: 0.4249, Acc: 88.73%\n",
      "Train Batch: 500/573, Loss: 0.2538, Acc: 88.74%\n",
      "Test set: Average loss: 0.3565, Accuracy: 89.64%\n",
      "Epoch: 81/100\n",
      "Train Loss: 0.3854, Train Acc: 88.79%\n",
      "Test Loss: 0.3565, Test Acc: 89.64%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4234, Acc: 84.38%\n",
      "Train Batch: 100/573, Loss: 0.3509, Acc: 89.17%\n",
      "Train Batch: 200/573, Loss: 0.3643, Acc: 88.83%\n",
      "Train Batch: 300/573, Loss: 0.4975, Acc: 88.87%\n",
      "Train Batch: 400/573, Loss: 0.4357, Acc: 88.81%\n",
      "Train Batch: 500/573, Loss: 0.2736, Acc: 88.72%\n",
      "Test set: Average loss: 0.3560, Accuracy: 89.81%\n",
      "Epoch: 82/100\n",
      "Train Loss: 0.3886, Train Acc: 88.68%\n",
      "Test Loss: 0.3560, Test Acc: 89.81%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3735, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.4248, Acc: 88.16%\n",
      "Train Batch: 200/573, Loss: 0.2782, Acc: 88.50%\n",
      "Train Batch: 300/573, Loss: 0.4081, Acc: 88.65%\n",
      "Train Batch: 400/573, Loss: 0.3955, Acc: 88.60%\n",
      "Train Batch: 500/573, Loss: 0.4436, Acc: 88.63%\n",
      "Test set: Average loss: 0.3575, Accuracy: 89.79%\n",
      "Epoch: 83/100\n",
      "Train Loss: 0.3893, Train Acc: 88.67%\n",
      "Test Loss: 0.3575, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4570, Acc: 85.16%\n",
      "Train Batch: 100/573, Loss: 0.4761, Acc: 89.00%\n",
      "Train Batch: 200/573, Loss: 0.3051, Acc: 88.77%\n",
      "Train Batch: 300/573, Loss: 0.4135, Acc: 88.62%\n",
      "Train Batch: 400/573, Loss: 0.4734, Acc: 88.58%\n",
      "Train Batch: 500/573, Loss: 0.2760, Acc: 88.55%\n",
      "Test set: Average loss: 0.3547, Accuracy: 89.77%\n",
      "Epoch: 84/100\n",
      "Train Loss: 0.3941, Train Acc: 88.56%\n",
      "Test Loss: 0.3547, Test Acc: 89.77%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2718, Acc: 92.19%\n",
      "Train Batch: 100/573, Loss: 0.2585, Acc: 88.69%\n",
      "Train Batch: 200/573, Loss: 0.3874, Acc: 88.70%\n",
      "Train Batch: 300/573, Loss: 0.4529, Acc: 88.81%\n",
      "Train Batch: 400/573, Loss: 0.5534, Acc: 88.69%\n",
      "Train Batch: 500/573, Loss: 0.5688, Acc: 88.64%\n",
      "Test set: Average loss: 0.3578, Accuracy: 89.63%\n",
      "Epoch: 85/100\n",
      "Train Loss: 0.3923, Train Acc: 88.64%\n",
      "Test Loss: 0.3578, Test Acc: 89.63%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3429, Acc: 86.72%\n",
      "Train Batch: 100/573, Loss: 0.3715, Acc: 89.01%\n",
      "Train Batch: 200/573, Loss: 0.5136, Acc: 88.99%\n",
      "Train Batch: 300/573, Loss: 0.3438, Acc: 88.97%\n",
      "Train Batch: 400/573, Loss: 0.3572, Acc: 88.79%\n",
      "Train Batch: 500/573, Loss: 0.4628, Acc: 88.85%\n",
      "Test set: Average loss: 0.3549, Accuracy: 89.79%\n",
      "Epoch: 86/100\n",
      "Train Loss: 0.3884, Train Acc: 88.79%\n",
      "Test Loss: 0.3549, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4932, Acc: 82.03%\n",
      "Train Batch: 100/573, Loss: 0.3015, Acc: 88.58%\n",
      "Train Batch: 200/573, Loss: 0.4055, Acc: 88.53%\n",
      "Train Batch: 300/573, Loss: 0.3293, Acc: 88.47%\n",
      "Train Batch: 400/573, Loss: 0.3278, Acc: 88.53%\n",
      "Train Batch: 500/573, Loss: 0.2642, Acc: 88.59%\n",
      "Test set: Average loss: 0.3537, Accuracy: 89.75%\n",
      "Epoch: 87/100\n",
      "Train Loss: 0.3906, Train Acc: 88.62%\n",
      "Test Loss: 0.3537, Test Acc: 89.75%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2483, Acc: 92.97%\n",
      "Train Batch: 100/573, Loss: 0.2138, Acc: 88.86%\n",
      "Train Batch: 200/573, Loss: 0.3047, Acc: 88.67%\n",
      "Train Batch: 300/573, Loss: 0.2496, Acc: 88.77%\n",
      "Train Batch: 400/573, Loss: 0.4912, Acc: 88.70%\n",
      "Train Batch: 500/573, Loss: 0.4285, Acc: 88.69%\n",
      "Test set: Average loss: 0.3593, Accuracy: 89.67%\n",
      "Epoch: 88/100\n",
      "Train Loss: 0.3886, Train Acc: 88.71%\n",
      "Test Loss: 0.3593, Test Acc: 89.67%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3604, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.6720, Acc: 88.92%\n",
      "Train Batch: 200/573, Loss: 0.3983, Acc: 88.84%\n",
      "Train Batch: 300/573, Loss: 0.3164, Acc: 88.94%\n",
      "Train Batch: 400/573, Loss: 0.5394, Acc: 88.81%\n",
      "Train Batch: 500/573, Loss: 0.4670, Acc: 88.76%\n",
      "Test set: Average loss: 0.3484, Accuracy: 89.95%\n",
      "Epoch: 89/100\n",
      "Train Loss: 0.3870, Train Acc: 88.78%\n",
      "Test Loss: 0.3484, Test Acc: 89.95%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3718, Acc: 89.06%\n",
      "Train Batch: 100/573, Loss: 0.3385, Acc: 88.99%\n",
      "Train Batch: 200/573, Loss: 0.2740, Acc: 88.87%\n",
      "Train Batch: 300/573, Loss: 0.3179, Acc: 88.75%\n",
      "Train Batch: 400/573, Loss: 0.4410, Acc: 88.69%\n",
      "Train Batch: 500/573, Loss: 0.3578, Acc: 88.76%\n",
      "Test set: Average loss: 0.3568, Accuracy: 89.68%\n",
      "Epoch: 90/100\n",
      "Train Loss: 0.3886, Train Acc: 88.66%\n",
      "Test Loss: 0.3568, Test Acc: 89.68%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5646, Acc: 85.16%\n",
      "Train Batch: 100/573, Loss: 0.2615, Acc: 88.81%\n",
      "Train Batch: 200/573, Loss: 0.5220, Acc: 88.66%\n",
      "Train Batch: 300/573, Loss: 0.4671, Acc: 88.55%\n",
      "Train Batch: 400/573, Loss: 0.3289, Acc: 88.68%\n",
      "Train Batch: 500/573, Loss: 0.6528, Acc: 88.73%\n",
      "Test set: Average loss: 0.3556, Accuracy: 89.85%\n",
      "Epoch: 91/100\n",
      "Train Loss: 0.3892, Train Acc: 88.70%\n",
      "Test Loss: 0.3556, Test Acc: 89.85%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3142, Acc: 92.97%\n",
      "Train Batch: 100/573, Loss: 0.4518, Acc: 88.70%\n",
      "Train Batch: 200/573, Loss: 0.3373, Acc: 88.62%\n",
      "Train Batch: 300/573, Loss: 0.5819, Acc: 88.67%\n",
      "Train Batch: 400/573, Loss: 0.3456, Acc: 88.73%\n",
      "Train Batch: 500/573, Loss: 0.3702, Acc: 88.70%\n",
      "Test set: Average loss: 0.3537, Accuracy: 89.79%\n",
      "Epoch: 92/100\n",
      "Train Loss: 0.3870, Train Acc: 88.72%\n",
      "Test Loss: 0.3537, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4048, Acc: 89.84%\n",
      "Train Batch: 100/573, Loss: 0.3435, Acc: 88.79%\n",
      "Train Batch: 200/573, Loss: 0.3767, Acc: 88.64%\n",
      "Train Batch: 300/573, Loss: 0.3476, Acc: 88.82%\n",
      "Train Batch: 400/573, Loss: 0.3750, Acc: 88.76%\n",
      "Train Batch: 500/573, Loss: 0.2455, Acc: 88.85%\n",
      "Test set: Average loss: 0.3522, Accuracy: 89.82%\n",
      "Epoch: 93/100\n",
      "Train Loss: 0.3883, Train Acc: 88.79%\n",
      "Test Loss: 0.3522, Test Acc: 89.82%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.5561, Acc: 85.94%\n",
      "Train Batch: 100/573, Loss: 0.2979, Acc: 88.84%\n",
      "Train Batch: 200/573, Loss: 0.4179, Acc: 88.91%\n",
      "Train Batch: 300/573, Loss: 0.3539, Acc: 88.88%\n",
      "Train Batch: 400/573, Loss: 0.3449, Acc: 88.94%\n",
      "Train Batch: 500/573, Loss: 0.3536, Acc: 88.92%\n",
      "Test set: Average loss: 0.3537, Accuracy: 89.77%\n",
      "Epoch: 94/100\n",
      "Train Loss: 0.3843, Train Acc: 88.93%\n",
      "Test Loss: 0.3537, Test Acc: 89.77%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2889, Acc: 92.19%\n",
      "Train Batch: 100/573, Loss: 0.4384, Acc: 89.12%\n",
      "Train Batch: 200/573, Loss: 0.3802, Acc: 88.84%\n",
      "Train Batch: 300/573, Loss: 0.3211, Acc: 88.60%\n",
      "Train Batch: 400/573, Loss: 0.3071, Acc: 88.56%\n",
      "Train Batch: 500/573, Loss: 0.4542, Acc: 88.60%\n",
      "Test set: Average loss: 0.3542, Accuracy: 89.76%\n",
      "Epoch: 95/100\n",
      "Train Loss: 0.3896, Train Acc: 88.67%\n",
      "Test Loss: 0.3542, Test Acc: 89.76%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2495, Acc: 92.19%\n",
      "Train Batch: 100/573, Loss: 0.3690, Acc: 89.04%\n",
      "Train Batch: 200/573, Loss: 0.3846, Acc: 88.88%\n",
      "Train Batch: 300/573, Loss: 0.4224, Acc: 88.75%\n",
      "Train Batch: 400/573, Loss: 0.3669, Acc: 88.79%\n",
      "Train Batch: 500/573, Loss: 0.4051, Acc: 88.81%\n",
      "Test set: Average loss: 0.3557, Accuracy: 89.79%\n",
      "Epoch: 96/100\n",
      "Train Loss: 0.3854, Train Acc: 88.80%\n",
      "Test Loss: 0.3557, Test Acc: 89.79%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.3907, Acc: 87.50%\n",
      "Train Batch: 100/573, Loss: 0.5785, Acc: 88.58%\n",
      "Train Batch: 200/573, Loss: 0.4106, Acc: 88.83%\n",
      "Train Batch: 300/573, Loss: 0.3993, Acc: 88.88%\n",
      "Train Batch: 400/573, Loss: 0.4245, Acc: 88.69%\n",
      "Train Batch: 500/573, Loss: 0.4111, Acc: 88.67%\n",
      "Test set: Average loss: 0.3563, Accuracy: 89.69%\n",
      "Epoch: 97/100\n",
      "Train Loss: 0.3895, Train Acc: 88.66%\n",
      "Test Loss: 0.3563, Test Acc: 89.69%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4232, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.3788, Acc: 88.78%\n",
      "Train Batch: 200/573, Loss: 0.4572, Acc: 88.58%\n",
      "Train Batch: 300/573, Loss: 0.2611, Acc: 88.72%\n",
      "Train Batch: 400/573, Loss: 0.4334, Acc: 88.68%\n",
      "Train Batch: 500/573, Loss: 0.4971, Acc: 88.67%\n",
      "Test set: Average loss: 0.3544, Accuracy: 89.74%\n",
      "Epoch: 98/100\n",
      "Train Loss: 0.3889, Train Acc: 88.64%\n",
      "Test Loss: 0.3544, Test Acc: 89.74%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.4990, Acc: 85.16%\n",
      "Train Batch: 100/573, Loss: 0.3839, Acc: 88.58%\n",
      "Train Batch: 200/573, Loss: 0.3122, Acc: 88.52%\n",
      "Train Batch: 300/573, Loss: 0.4660, Acc: 88.39%\n",
      "Train Batch: 400/573, Loss: 0.4513, Acc: 88.50%\n",
      "Train Batch: 500/573, Loss: 0.4178, Acc: 88.58%\n",
      "Test set: Average loss: 0.3540, Accuracy: 89.68%\n",
      "Epoch: 99/100\n",
      "Train Loss: 0.3917, Train Acc: 88.57%\n",
      "Test Loss: 0.3540, Test Acc: 89.68%\n",
      "------------------------------------------------------------\n",
      "Train Batch: 0/573, Loss: 0.2680, Acc: 90.62%\n",
      "Train Batch: 100/573, Loss: 0.4487, Acc: 88.58%\n",
      "Train Batch: 200/573, Loss: 0.2072, Acc: 88.55%\n",
      "Train Batch: 300/573, Loss: 0.2671, Acc: 88.73%\n",
      "Train Batch: 400/573, Loss: 0.6124, Acc: 88.53%\n",
      "Train Batch: 500/573, Loss: 0.4319, Acc: 88.63%\n",
      "Test set: Average loss: 0.3526, Accuracy: 89.87%\n",
      "Epoch: 100/100\n",
      "Train Loss: 0.3895, Train Acc: 88.64%\n",
      "Test Loss: 0.3526, Test Acc: 89.87%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Improved data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))  # SVHN specific stats\n",
    "])\n",
    "\n",
    "train_dataset = datasets.SVHN(root='./data', split='train', transform=transform, download=True)\n",
    "test_dataset = datasets.SVHN(root='./data', split='test', transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "class FlipoutLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.prior_sigma = prior_sigma\n",
    "        \n",
    "        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features).normal_(0, 0.01))\n",
    "        self.weight_rho = nn.Parameter(torch.zeros(out_features, in_features).normal_(-5, 0.01))\n",
    "        self.bias_mu = nn.Parameter(torch.zeros(out_features).normal_(0, 0.01))\n",
    "        self.bias_rho = nn.Parameter(torch.zeros(out_features).normal_(-5, 0.01))\n",
    "        \n",
    "        # Random signs for flipout\n",
    "        self.register_buffer('input_sign', None)\n",
    "        self.register_buffer('output_sign', None)\n",
    "        \n",
    "    def get_random_signs(self, batch_size, shape, device):\n",
    "        return (2 * torch.bernoulli(torch.ones(batch_size, *shape, device=device) * 0.5) - 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if (self.input_sign is None or \n",
    "            self.input_sign.size(0) != batch_size or \n",
    "            self.output_sign is None or \n",
    "            self.output_sign.size(0) != batch_size):\n",
    "            \n",
    "            self.input_sign = self.get_random_signs(batch_size, (self.in_features,), x.device)\n",
    "            self.output_sign = self.get_random_signs(batch_size, (self.out_features,), x.device)\n",
    "        \n",
    "        weight_sigma = F.softplus(self.weight_rho)\n",
    "        bias_sigma = F.softplus(self.bias_rho)\n",
    "        \n",
    "        mean_output = F.linear(x, self.weight_mu, self.bias_mu)\n",
    "        \n",
    "        # Compute perturbation with corrected dimensions\n",
    "        r_w = torch.randn_like(self.weight_mu)\n",
    "        perturbed_weights = (r_w * weight_sigma).unsqueeze(0)\n",
    "        \n",
    "        x_reshape = x * self.input_sign\n",
    "        perturbation = F.linear(x_reshape, perturbed_weights.squeeze(0)) * self.output_sign\n",
    "        \n",
    "        bias_perturbation = bias_sigma * torch.randn_like(self.bias_mu)\n",
    "        \n",
    "        return mean_output + perturbation + bias_perturbation\n",
    "\n",
    "    def kl_loss(self):\n",
    "        weight_sigma = F.softplus(self.weight_rho)\n",
    "        bias_sigma = F.softplus(self.bias_rho)\n",
    "        \n",
    "        kl_weight = 0.5 * torch.sum(\n",
    "            torch.log1p((weight_sigma**2) / (self.prior_sigma**2)) +\n",
    "            (self.weight_mu**2) / (self.prior_sigma**2) - 1\n",
    "        )\n",
    "        \n",
    "        kl_bias = 0.5 * torch.sum(\n",
    "            torch.log1p((bias_sigma**2) / (self.prior_sigma**2)) +\n",
    "            (self.bias_mu**2) / (self.prior_sigma**2) - 1\n",
    "        )\n",
    "        \n",
    "        return (kl_weight + kl_bias) / self.weight_mu.numel()\n",
    "\n",
    "class BayesianNNFlipout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = FlipoutLinear(64 * 4 * 4, 512, prior_sigma=0.1)\n",
    "        self.fc2 = FlipoutLinear(512, 10, prior_sigma=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def kl_loss(self):\n",
    "        return self.fc1.kl_loss() + self.fc2.kl_loss()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, epoch, total_epochs=100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    beta = min(1.0, (epoch / (total_epochs * 0.2))) * 0.1\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        nll_loss = F.cross_entropy(output, target)\n",
    "        kl_loss = model.kl_loss()\n",
    "        loss = nll_loss + beta * kl_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Batch: {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'Acc: {100. * correct/total:.2f}%')\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {100. * accuracy:.2f}%')\n",
    "    \n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = BayesianNNFlipout().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# Training loop with beta warmup\n",
    "n_epochs = 100\n",
    "beta_warmup_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    beta = min(1.0, epoch / beta_warmup_epochs)  # Linear warmup of KL term\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, beta)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {100*train_acc:.2f}%')\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {100*test_acc:.2f}%')\n",
    "    print('-' * 60)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'svhn_results_4000_samples_VI.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
